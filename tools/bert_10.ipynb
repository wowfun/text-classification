{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考https://github.com/NZbryan/NLP_bert\n",
    "##去https://huggingface.co/bert-base-chinese 下载tf_model.h5到./pre_model/\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from transformers import TFBertForSequenceClassification,BertTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from utils import data_helper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_class_dic={0: '家居', 1: '房产', 2: '教育', 3: '时尚', 4: '时政', 5: '科技', 6: '财经',7: '游戏',8: '娱乐',9: '体育'}\n",
    "def getY(line):\n",
    "    return cn_class_dic[line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text  y label\n",
       "1072  人民网 北京 9 月 8 日电 孙竞 9 月 7 日 北京市 52 万名 二至 四年级 学生...  2    教育\n",
       "4379  1 基本面 2 月 马 棕 库存 下降 42MPOB 报告 利多 船调 机构 显示 本月 马...  6    财经\n",
       "3761  9 月 11 日 广电总局 公示 8 月 全国 拍摄 制作 电视剧 备案 公示 显示 爱情 ...  8    娱乐\n",
       "778   光明日报 记者 周仕兴 光明日报 通讯员 谢曼妮 一玉口 中国 一瓦顶 成家 都 说国 很大...  2    教育\n",
       "3386  天涯 明月刀 手游 中 NPC 打好 关系 想要 NPC 打好 关系 肯定 送礼 一个 正确...  7    游戏\n",
       "2540  一梦 江湖 一款 好玩 游戏 优秀 画风 优质 剧情 吸引 不少 玩家 很多 新手 玩家 不...  7    游戏\n",
       "7212  做 期货 配资 心态 很 交易 习惯 带来 建议 期货市场 中 避开 陷阱 T0 交易 频繁...  6    财经\n",
       "3593  2020 高校 招生 服务 光明 大 直播 光明日报 记者 苏雁 赵秋丽 冯帆 吴琳 刘勇 ...  2    教育\n",
       "3234  期货 滚动 结算 滚动 结算 交易日 时间 内 结算 次数 不同于 商品 期货市场 每日 收...  6    财经\n",
       "7535  8 月 6 日安兔 兔 官方 发布 2020 年 7 月份 价位 段 手机 性价比 排行榜 ...  5    科技"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>y</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1072</th>\n      <td>人民网 北京 9 月 8 日电 孙竞 9 月 7 日 北京市 52 万名 二至 四年级 学生...</td>\n      <td>2</td>\n      <td>教育</td>\n    </tr>\n    <tr>\n      <th>4379</th>\n      <td>1 基本面 2 月 马 棕 库存 下降 42MPOB 报告 利多 船调 机构 显示 本月 马...</td>\n      <td>6</td>\n      <td>财经</td>\n    </tr>\n    <tr>\n      <th>3761</th>\n      <td>9 月 11 日 广电总局 公示 8 月 全国 拍摄 制作 电视剧 备案 公示 显示 爱情 ...</td>\n      <td>8</td>\n      <td>娱乐</td>\n    </tr>\n    <tr>\n      <th>778</th>\n      <td>光明日报 记者 周仕兴 光明日报 通讯员 谢曼妮 一玉口 中国 一瓦顶 成家 都 说国 很大...</td>\n      <td>2</td>\n      <td>教育</td>\n    </tr>\n    <tr>\n      <th>3386</th>\n      <td>天涯 明月刀 手游 中 NPC 打好 关系 想要 NPC 打好 关系 肯定 送礼 一个 正确...</td>\n      <td>7</td>\n      <td>游戏</td>\n    </tr>\n    <tr>\n      <th>2540</th>\n      <td>一梦 江湖 一款 好玩 游戏 优秀 画风 优质 剧情 吸引 不少 玩家 很多 新手 玩家 不...</td>\n      <td>7</td>\n      <td>游戏</td>\n    </tr>\n    <tr>\n      <th>7212</th>\n      <td>做 期货 配资 心态 很 交易 习惯 带来 建议 期货市场 中 避开 陷阱 T0 交易 频繁...</td>\n      <td>6</td>\n      <td>财经</td>\n    </tr>\n    <tr>\n      <th>3593</th>\n      <td>2020 高校 招生 服务 光明 大 直播 光明日报 记者 苏雁 赵秋丽 冯帆 吴琳 刘勇 ...</td>\n      <td>2</td>\n      <td>教育</td>\n    </tr>\n    <tr>\n      <th>3234</th>\n      <td>期货 滚动 结算 滚动 结算 交易日 时间 内 结算 次数 不同于 商品 期货市场 每日 收...</td>\n      <td>6</td>\n      <td>财经</td>\n    </tr>\n    <tr>\n      <th>7535</th>\n      <td>8 月 6 日安兔 兔 官方 发布 2020 年 7 月份 价位 段 手机 性价比 排行榜 ...</td>\n      <td>5</td>\n      <td>科技</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_ds='../data/train/train_processed_v1.csv'\n",
    "train_df=pd.read_csv(train_ds)[['cutted_content','label']]\n",
    "train_df.columns=['content','label']\n",
    "\n",
    "train_raw = train_df[['content','label']]\n",
    "\n",
    "train_raw.columns=['text','y']\n",
    "train_raw['label']=train_raw['y'].apply(getY)\n",
    "train_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at ./pre_model were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at ./pre_model and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  102267648 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  7690      \n",
      "=================================================================\n",
      "Total params: 102,275,338\n",
      "Trainable params: 102,275,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#建立模型\n",
    "num_classes=10\n",
    "learning_rate=2e-5\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('./pre_model', num_labels=num_classes)\n",
    "\n",
    "# optimizer Adam recommended\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "  \n",
    "  # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length\n",
    "\treturn tokenizer.encode_plus(review, \n",
    "\t            add_special_tokens = True, # add [CLS], [SEP]\n",
    "\t            max_length = max_length, # max length of the text that can go to BERT\n",
    "\t            pad_to_max_length = True, # add [PAD] tokens\n",
    "\t            return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "\t\t    truncation=True\n",
    "\t          )\n",
    "# map to the expected input to TFBertForSequenceClassification, see here \n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def encode_examples(ds, limit=-1):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "  \n",
    "    for index, row in ds.iterrows():\n",
    "        review = row[\"text\"]\n",
    "        label = row[\"y\"]\n",
    "        bert_input = convert_example_to_feature(review)\n",
    "  \n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset(df):\n",
    "    train_set, x = train_test_split(df, \n",
    "        stratify=df['label'],\n",
    "        test_size=0.1, \n",
    "        random_state=42)\n",
    "    val_set, test_set = train_test_split(x, \n",
    "        stratify=x['label'],\n",
    "        test_size=0.5, \n",
    "        random_state=43)\n",
    "\n",
    "    return train_set,val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#预处理数据\n",
    "\n",
    "model_path = './pre_model/bert-base-chinese-vocab.txt' #模型路径，建议预先下载(https://huggingface.co/bert-base-chinese#)\n",
    "\n",
    "max_length = 64\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "number_of_epochs = 1\n",
    "num_classes = 10 # 类别数\n",
    "\n",
    "\n",
    "# split data\n",
    "train_data,val_data, test_data = split_dataset(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# train dataset\n",
    "ds_train_encoded = encode_examples(train_data).shuffle(10000).batch(batch_size)\n",
    "# val dataset\n",
    "ds_val_encoded = encode_examples(val_data).batch(batch_size)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(test_data).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "563/563 [==============================] - 3205s 6s/step - loss: 0.5930 - accuracy: 0.8524 - val_loss: 0.3551 - val_accuracy: 0.9040\n",
      "32/32 [==============================] - 47s 1s/step - loss: 0.3662 - accuracy: 0.9000\n",
      "# evaluate test_set: [0.3661502003669739, 0.8999999761581421]\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_val_encoded)\n",
    "# evaluate test_set\n",
    "print(\"# evaluate test_set:\",model.evaluate(ds_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('bert_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at ./pre_model were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at ./pre_model and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  102267648 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  7690      \n",
      "=================================================================\n",
      "Total params: 102,275,338\n",
      "Trainable params: 102,275,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#建立模型\n",
    "num_classes=10\n",
    "learning_rate=2e-5\n",
    "\n",
    "new_model = TFBertForSequenceClassification.from_pretrained('./pre_model', num_labels=num_classes)\n",
    "\n",
    "# optimizer Adam recommended\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "new_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "print(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f672c4ced60>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "new_model.load_weights('bert_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在测试集上输出结果\n",
    "test_ds='../data/test_data.csv'\n",
    "test_df=pd.read_csv(test_ds)[['content']]\n",
    "## 大于4k 长度的比例\n",
    "#print(len(test_df[test_df['content'].str.len()>4000]['content'])/len(test_df))\n",
    "## 截断到 4k\n",
    "idxs=test_df.loc[test_df['content'].str.len()>4000].index\n",
    "#print(test_df.loc[test_df['content'].str.len()>4000].index)\n",
    "test_df.loc[test_df['content'].str.len()>4000,'content']=test_df.loc[test_df['content'].str.len()>4000,'content'].apply(lambda x:x[:4000].rsplit('。',1)[0])\n",
    "\n",
    "#删除除字母,数字，汉字以外的所有符号删除除字母,数字，汉字以外的所有符号\n",
    "test_df['cleaned_content']=test_df['content'].apply(data_helper.remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_test(line):\n",
    "    return np.array(tokenizer.encode(line,max_length = max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "model_path = './pre_model/bert-base-chinese-vocab.txt'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "test_encode=test_df['cleaned_content'].apply(encode_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "test_encode.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y=[]\n",
    "for i in range(test_encode.size):\n",
    "    #tmp=np.array(new_model.predict([test_encode[i]])[0][0])\n",
    "    predict_y.append(new_model.predict([test_encode[i]])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([7, 9, 0, 0, 7, 5, 9, 3, 4, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "predict_y=np.array(predict_y)\n",
    "y_pred1 = predict_y.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{0: '家居', 1: '房产', 2: '教育', 3: '时尚', 4: '时政', 5: '科技', 6: '财经',7: '游戏',8: '娱乐',9: '体育'}\n",
    "#7类，分别为：财经、房产、家居、教育、科技、时尚、时政\n",
    "#10个类别:财经、房产、家居、教育、科技、时尚、时政、游戏、娱乐、体育。\n",
    "#{0: '可公开', 1: '低风险', 2: '中风险', 3: '高风险'}\n",
    "#rank_label对应关系:0{0,8,9} 1{2,3,7} 2{1,5} 3{4,6}\n",
    "def reflect_rank(line):\n",
    "    res=-1\n",
    "    if(line==0 or line==8 or line==9):\n",
    "        res=0\n",
    "    elif(line==2 or line==3 or line==7):\n",
    "        res=1\n",
    "    elif(line==1 or line==5):\n",
    "        res=2\n",
    "    elif(line==4 or line==6):\n",
    "        res=3\n",
    "    return res\n",
    "\n",
    "cn_class_dic={0: '家居', 1: '房产', 2: '教育', 3: '时尚', 4: '时政', 5: '科技', 6: '财经',7: '游戏',8: '娱乐',9: '体育'}\n",
    "cn_rank_dic={0: '可公开', 1: '低风险', 2: '中风险', 3: '高风险'}\n",
    "\n",
    "def cn_class_label(line):\n",
    "    return cn_class_dic[line]\n",
    "\n",
    "def cn_rank_label(line):\n",
    "    return cn_rank_dic[line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id class_label rank_label\n",
    "\n",
    "test_pr=pd.DataFrame(y_pred1)\n",
    "test_pr.columns = ['class_label']\n",
    "test_pr['rank_label']=test_pr['class_label'].apply(reflect_rank)\n",
    "test_pr['class_label']=test_pr['class_label'].apply(cn_class_label)\n",
    "test_pr['rank_label']=test_pr['rank_label'].apply(cn_rank_label)#id class_label rank_label\n",
    "\n",
    "test_pr=pd.DataFrame(y_pred1)\n",
    "test_pr.columns = ['class_label']\n",
    "test_pr['rank_label']=test_pr['class_label'].apply(reflect_rank)\n",
    "test_pr['class_label']=test_pr['class_label'].apply(cn_class_label)\n",
    "test_pr['rank_label']=test_pr['rank_label'].apply(cn_rank_label)\n",
    "test_pr.to_csv('submission_bert_10.csv',index_label='id')"
   ]
  }
 ]
}